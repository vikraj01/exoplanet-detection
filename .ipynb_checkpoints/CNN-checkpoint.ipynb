{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ba60e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import uniform_filter1d, gaussian_filter  # Updated import statements\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    confusion_matrix, fbeta_score, precision_recall_curve,\n",
    "    average_precision_score, auc\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "009e59d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv1D, MaxPool1D, Dense, Dropout, Flatten, \\\n",
    "    BatchNormalization, Input, concatenate, Activation\n",
    "from keras.optimizers import Adam\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from inspect import signature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7403939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define detrender function\n",
    "def detrender_normalizer(light_flux):\n",
    "  flux1 = light_flux\n",
    "  flux2 = gaussian_filter(flux1, sigma=10)\n",
    "  flux3 = flux1 - flux2\n",
    "  flux3normalized = (flux3-np.mean(flux3)) / (np.max(flux3)-np.min(flux3))\n",
    "  return flux3normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3f9964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function for shuffeling in unison\n",
    "def shuffle_in_unison(a, b):\n",
    "  rng_state = np.random.get_state()\n",
    "  np.random.shuffle(a)\n",
    "  np.random.set_state(rng_state)\n",
    "  np.random.shuffle(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3db2940a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(x_train, y_train, batch_size=32):\n",
    "    half_batch = batch_size // 2\n",
    "    x_batch = np.empty((batch_size, x_train.shape[1], x_train.shape[2]), dtype='float32') #empty matrix for input\n",
    "    y_batch = np.empty((batch_size, y_train.shape[1]), dtype='float32') #empty matrix for output\n",
    "\n",
    "    # Find indicies for positive and negative labels\n",
    "    while True:\n",
    "        pos_idx = np.where(y_train[:,0] == 1)[0]\n",
    "        neg_idx = np.where(y_train[:,0] == 0)[0]\n",
    "\n",
    "        # Randomize the positive and negative indicies\n",
    "        np.random.shuffle(pos_idx)\n",
    "        np.random.shuffle(neg_idx)\n",
    "\n",
    "        # Let half of the batch have a positive classification and the other\n",
    "        # half have a negative classification\n",
    "        x_batch[:half_batch] = x_train[pos_idx[:half_batch]]\n",
    "        x_batch[half_batch:] = x_train[neg_idx[half_batch:batch_size]] \n",
    "        y_batch[:half_batch] = y_train[pos_idx[:half_batch]]\n",
    "        y_batch[half_batch:] = y_train[neg_idx[half_batch:batch_size]]\n",
    "\n",
    "        # Shuffle batch\n",
    "        shuffle_in_unison(x_batch,y_batch)\n",
    "\n",
    "        # Generating new examples by rotating them in time\n",
    "        for i in range(batch_size):\n",
    "            sz = np.random.randint(x_batch.shape[1])\n",
    "            x_batch[i] = np.roll(x_batch[i], sz, axis = 0)\n",
    "        yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a30b9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Converting the formate from dataframe to numpy arrays (matrices)\n",
    "    # and defining x-values and y-values for both the test and training set\n",
    "    print(\"Loading datasets...\")\n",
    "    train = pd.read_csv(\"Data/exoTrain.csv\", encoding= \"ISO-8859-1\") #on data frame format\n",
    "    test = pd.read_csv(\"Data/exoTest.csv\", encoding= \"ISO-8859-1\") #on data frame format\n",
    "    x_train = train.drop('LABEL', axis=1)\n",
    "    x_test = test.drop('LABEL', axis=1)\n",
    "    y_train = train.LABEL\n",
    "    y_test = test.LABEL\n",
    "    x_train = np.array(x_train)\n",
    "    y_train = np.array(y_train).reshape((-1,1))-1\n",
    "    x_test = np.array(x_test)\n",
    "    y_test = np.array(y_test).reshape((-1,1))-1 \n",
    "    \n",
    "    # Add extra positive examples of light curves by flipping them\n",
    "    x_train = np.append(x_train, np.flip(x_train[0:37,:], axis=-1), axis=0)\n",
    "    y_train = np.append(y_train, y_train[0:37]).reshape((-1,1))\n",
    "    x_test = np.append(x_test, np.flip(x_test[0:5,:], axis=-1), axis=0)\n",
    "    y_test = np.append(y_test, y_test[0:5]).reshape((-1,1))\n",
    "    \n",
    "    # Detrend the data sets\n",
    "    x_train_p = detrender_normalizer(x_train)\n",
    "    x_test_p = detrender_normalizer(x_test)\n",
    "    \n",
    "    # Scale each observation to zero mean and unit variance\n",
    "    x_train = ((x_train - np.mean(x_train, axis=1).reshape(-1,1)) / np.std(x_train, axis=1).reshape(-1,1))\n",
    "    x_test = ((x_test - np.mean(x_test, axis=1).reshape(-1,1)) / np.std(x_test, axis=1).reshape(-1,1))\n",
    "  \n",
    "    # Stack the zero mean unit variance normalized data and the detrended data\n",
    "    x_train = np.stack([x_train, x_train_p], axis=2)\n",
    "    x_test = np.stack([x_test, x_test_p], axis=2)\n",
    "    \n",
    "    print(\"Constructing The Neural Networks...\")\n",
    "    # Construct the neural network\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=8, kernel_size=11, activation='linear', input_shape=x_train.shape[1:]))\n",
    "    model.add(MaxPool1D(strides=4))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=16, kernel_size=11, activation='relu'))\n",
    "    model.add(MaxPool1D(strides=4))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=32, kernel_size=11, activation='relu'))\n",
    "    model.add(MaxPool1D(strides=4))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=64, kernel_size=11, activation='relu'))\n",
    "    model.add(MaxPool1D(strides=4))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5)) #prevents overfitting\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.25)) #prevents overfitting\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model and train the model, make sure it converges\n",
    "    model.compile(optimizer=Adam(1e-5), loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "    hist = model.fit_generator(batch_generator(x_train, y_train, 32), \\\n",
    "                                validation_data=(x_test, y_test), \\\n",
    "                                verbose=0, epochs=5, \\\n",
    "                                steps_per_epoch=x_train.shape[0]//32)\n",
    "\n",
    "    # Proceeding the training with faster learning rate\n",
    "    model.compile(optimizer=Adam(4e-5), loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "    hist = model.fit_generator(batch_generator(x_train, y_train, 32), \n",
    "                                validation_data=(x_test, y_test), \n",
    "                                verbose=2, epochs=50,\n",
    "                                steps_per_epoch=x_train.shape[0]//32)\n",
    "\n",
    "    # Saving model to JSON and weights to HDF5\n",
    "    model_json = model.to_json()\n",
    "    with open(\"model.json\", \"w\") as  json_file:\n",
    "      json_file.write(model_json)\n",
    "    model.save_weights(\"model.h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "    # Plot the loss and accuracy for the training\n",
    "    plt.plot(hist.history['loss'], color='b',label='loss')\n",
    "    plt.plot(hist.history['val_loss'], color='r',label='validation loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    plt.plot(hist.history['acc'], color='b',label='accuracy')\n",
    "    plt.plot(hist.history['val_acc'], color='r',label='validation accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "    #Make predictions for training data\n",
    "    shuffle_in_unison(x_train,y_train)\n",
    "    y_pred = model.predict(x_train)[:,0]\n",
    "    pred = np.empty((1,len(y_pred)), dtype=object)\n",
    "    pred = np.where(y_pred>=0.5, 1, 0)\n",
    "    y_train = np.reshape(y_train,len(y_train))\n",
    "    pred = np.reshape(pred,len(pred))\n",
    "    \n",
    "    # Create confusion matrix for training data\n",
    "    print('Validation for training data:')\n",
    "    conf_matrix = pd.crosstab(y_train, pred)\n",
    "    print(conf_matrix)\n",
    "    \n",
    "    # Calculate precision and recall\n",
    "    accuracy = accuracy_score(y_train, pred)\n",
    "    precision = precision_score(y_train, pred)\n",
    "    recall = recall_score(y_train, pred)\n",
    "    fbeta = fbeta_score(y_train, pred, 1)\n",
    "    print('Accuracy: %.3f Precision: %.3f Recall: %.3f F_beta: %.3f' \\\n",
    "          % (accuracy, precision, recall, fbeta))\n",
    "    \n",
    "    # Create a precision recall curve\n",
    "    precision, recall, thresholds = precision_recall_curve(y_train, y_pred, pos_label=1)\n",
    "    auc_pr = auc(recall, precision)\n",
    "    print('Area under precision-recall-curve: %.3f' % (auc_pr))\n",
    "    step_kwargs = ({'step': 'post'}\n",
    "                   if 'step' in signature(plt.fill_between).parameters\n",
    "                   else {})\n",
    "    plt.step(recall, precision, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.show()\n",
    "\n",
    "    # Make predictions for test data\n",
    "    shuffle_in_unison(x_test,y_test)\n",
    "    y_pred = model.predict(x_test)[:,0] \n",
    "    pred = np.empty((1,len(y_pred)), dtype=object)\n",
    "    pred = np.where(y_pred>=0.5, 1, 0)\n",
    "    y_test = np.reshape(y_test,len(y_test))\n",
    "    pred = np.reshape(pred,len(pred))\n",
    "    \n",
    "    # Create confusion matrix for test data\n",
    "    print('Validation for test data:')\n",
    "    conf_matrix = pd.crosstab(y_test, pred)\n",
    "    print(conf_matrix)\n",
    "    \n",
    "    # Calculate precision and recall\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    precision = precision_score(y_test, pred)\n",
    "    recall = recall_score(y_test, pred)\n",
    "    fbeta = fbeta_score(y_test, pred, 1)\n",
    "    print('Accuracy: %.3f Precision: %.3f Recall: %.3f F_beta: %.3f' \\\n",
    "          % (accuracy, precision, recall, fbeta))\n",
    "    \n",
    "    # Create a precision recall curve\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred, pos_label=1)\n",
    "    auc_pr = auc(recall, precision)\n",
    "    print('Area under precision-recall-curve: %.3f' % (auc_pr))\n",
    "    step_kwargs = ({'step': 'post'}\n",
    "                   if 'step' in signature(plt.fill_between).parameters\n",
    "                   else {})\n",
    "    plt.step(recall, precision, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95b77849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Epoch 1/50\n",
      "160/160 - 6s - loss: 0.6681 - accuracy: 0.6025 - val_loss: 0.6279 - val_accuracy: 0.6626 - 6s/epoch - 38ms/step\n",
      "Epoch 2/50\n",
      "160/160 - 4s - loss: 0.6463 - accuracy: 0.6207 - val_loss: 0.6184 - val_accuracy: 0.6904 - 4s/epoch - 24ms/step\n",
      "Epoch 3/50\n",
      "160/160 - 4s - loss: 0.6380 - accuracy: 0.6389 - val_loss: 0.6097 - val_accuracy: 0.7061 - 4s/epoch - 24ms/step\n",
      "Epoch 4/50\n",
      "160/160 - 4s - loss: 0.6181 - accuracy: 0.6555 - val_loss: 0.6026 - val_accuracy: 0.7061 - 4s/epoch - 24ms/step\n",
      "Epoch 5/50\n",
      "160/160 - 4s - loss: 0.5990 - accuracy: 0.6697 - val_loss: 0.6110 - val_accuracy: 0.6974 - 4s/epoch - 24ms/step\n",
      "Epoch 6/50\n",
      "160/160 - 4s - loss: 0.5968 - accuracy: 0.6770 - val_loss: 0.5902 - val_accuracy: 0.7183 - 4s/epoch - 26ms/step\n",
      "Epoch 7/50\n",
      "160/160 - 4s - loss: 0.5831 - accuracy: 0.6945 - val_loss: 0.5828 - val_accuracy: 0.6991 - 4s/epoch - 25ms/step\n",
      "Epoch 8/50\n",
      "160/160 - 4s - loss: 0.5815 - accuracy: 0.6963 - val_loss: 0.5732 - val_accuracy: 0.6974 - 4s/epoch - 24ms/step\n",
      "Epoch 9/50\n",
      "160/160 - 4s - loss: 0.5678 - accuracy: 0.7100 - val_loss: 0.5832 - val_accuracy: 0.6783 - 4s/epoch - 24ms/step\n",
      "Epoch 10/50\n",
      "160/160 - 4s - loss: 0.5495 - accuracy: 0.7195 - val_loss: 0.5575 - val_accuracy: 0.6974 - 4s/epoch - 24ms/step\n",
      "Epoch 11/50\n",
      "160/160 - 4s - loss: 0.5405 - accuracy: 0.7326 - val_loss: 0.5553 - val_accuracy: 0.6887 - 4s/epoch - 24ms/step\n",
      "Epoch 12/50\n",
      "160/160 - 4s - loss: 0.5265 - accuracy: 0.7416 - val_loss: 0.5703 - val_accuracy: 0.6748 - 4s/epoch - 24ms/step\n",
      "Epoch 13/50\n",
      "160/160 - 4s - loss: 0.5260 - accuracy: 0.7449 - val_loss: 0.5320 - val_accuracy: 0.7078 - 4s/epoch - 25ms/step\n",
      "Epoch 14/50\n",
      "160/160 - 4s - loss: 0.5122 - accuracy: 0.7604 - val_loss: 0.5442 - val_accuracy: 0.6957 - 4s/epoch - 24ms/step\n",
      "Epoch 15/50\n",
      "160/160 - 4s - loss: 0.4856 - accuracy: 0.7729 - val_loss: 0.5041 - val_accuracy: 0.7061 - 4s/epoch - 25ms/step\n",
      "Epoch 16/50\n",
      "160/160 - 4s - loss: 0.4781 - accuracy: 0.7758 - val_loss: 0.5245 - val_accuracy: 0.6974 - 4s/epoch - 27ms/step\n",
      "Epoch 17/50\n",
      "160/160 - 4s - loss: 0.4564 - accuracy: 0.7867 - val_loss: 0.5333 - val_accuracy: 0.6887 - 4s/epoch - 27ms/step\n",
      "Epoch 18/50\n",
      "160/160 - 4s - loss: 0.4362 - accuracy: 0.8012 - val_loss: 0.4812 - val_accuracy: 0.7078 - 4s/epoch - 26ms/step\n",
      "Epoch 19/50\n",
      "160/160 - 4s - loss: 0.4111 - accuracy: 0.8152 - val_loss: 0.4591 - val_accuracy: 0.7287 - 4s/epoch - 26ms/step\n",
      "Epoch 20/50\n",
      "160/160 - 4s - loss: 0.3922 - accuracy: 0.8227 - val_loss: 0.4447 - val_accuracy: 0.7339 - 4s/epoch - 26ms/step\n",
      "Epoch 21/50\n",
      "160/160 - 4s - loss: 0.3705 - accuracy: 0.8391 - val_loss: 0.4485 - val_accuracy: 0.7496 - 4s/epoch - 26ms/step\n",
      "Epoch 22/50\n",
      "160/160 - 4s - loss: 0.3564 - accuracy: 0.8438 - val_loss: 0.4340 - val_accuracy: 0.7600 - 4s/epoch - 27ms/step\n",
      "Epoch 23/50\n",
      "160/160 - 5s - loss: 0.3338 - accuracy: 0.8578 - val_loss: 0.3647 - val_accuracy: 0.7948 - 5s/epoch - 29ms/step\n",
      "Epoch 24/50\n",
      "160/160 - 4s - loss: 0.3196 - accuracy: 0.8645 - val_loss: 0.3594 - val_accuracy: 0.7965 - 4s/epoch - 26ms/step\n",
      "Epoch 25/50\n",
      "160/160 - 4s - loss: 0.2998 - accuracy: 0.8742 - val_loss: 0.3532 - val_accuracy: 0.8052 - 4s/epoch - 26ms/step\n",
      "Epoch 26/50\n",
      "160/160 - 4s - loss: 0.2772 - accuracy: 0.8842 - val_loss: 0.3854 - val_accuracy: 0.8070 - 4s/epoch - 26ms/step\n",
      "Epoch 27/50\n",
      "160/160 - 4s - loss: 0.2792 - accuracy: 0.8869 - val_loss: 0.3347 - val_accuracy: 0.8261 - 4s/epoch - 28ms/step\n",
      "Epoch 28/50\n",
      "160/160 - 4s - loss: 0.2642 - accuracy: 0.8893 - val_loss: 0.3005 - val_accuracy: 0.8417 - 4s/epoch - 27ms/step\n",
      "Epoch 29/50\n",
      "160/160 - 4s - loss: 0.2677 - accuracy: 0.8887 - val_loss: 0.2955 - val_accuracy: 0.8452 - 4s/epoch - 27ms/step\n",
      "Epoch 30/50\n",
      "160/160 - 4s - loss: 0.2457 - accuracy: 0.8979 - val_loss: 0.2521 - val_accuracy: 0.8626 - 4s/epoch - 25ms/step\n",
      "Epoch 31/50\n",
      "160/160 - 4s - loss: 0.2221 - accuracy: 0.9084 - val_loss: 0.2616 - val_accuracy: 0.8643 - 4s/epoch - 27ms/step\n",
      "Epoch 32/50\n",
      "160/160 - 4s - loss: 0.2304 - accuracy: 0.9086 - val_loss: 0.3137 - val_accuracy: 0.8504 - 4s/epoch - 26ms/step\n",
      "Epoch 33/50\n",
      "160/160 - 4s - loss: 0.2205 - accuracy: 0.9072 - val_loss: 0.2554 - val_accuracy: 0.8800 - 4s/epoch - 25ms/step\n",
      "Epoch 34/50\n",
      "160/160 - 4s - loss: 0.2255 - accuracy: 0.9061 - val_loss: 0.2303 - val_accuracy: 0.8887 - 4s/epoch - 26ms/step\n",
      "Epoch 35/50\n",
      "160/160 - 4s - loss: 0.1985 - accuracy: 0.9176 - val_loss: 0.2481 - val_accuracy: 0.8835 - 4s/epoch - 24ms/step\n",
      "Epoch 36/50\n",
      "160/160 - 4s - loss: 0.1910 - accuracy: 0.9262 - val_loss: 0.2424 - val_accuracy: 0.8870 - 4s/epoch - 26ms/step\n",
      "Epoch 37/50\n",
      "160/160 - 4s - loss: 0.1921 - accuracy: 0.9240 - val_loss: 0.2255 - val_accuracy: 0.8887 - 4s/epoch - 25ms/step\n",
      "Epoch 38/50\n",
      "160/160 - 4s - loss: 0.1863 - accuracy: 0.9232 - val_loss: 0.2029 - val_accuracy: 0.9043 - 4s/epoch - 26ms/step\n",
      "Epoch 39/50\n",
      "160/160 - 4s - loss: 0.1929 - accuracy: 0.9215 - val_loss: 0.2254 - val_accuracy: 0.8974 - 4s/epoch - 27ms/step\n",
      "Epoch 40/50\n",
      "160/160 - 4s - loss: 0.1683 - accuracy: 0.9314 - val_loss: 0.2218 - val_accuracy: 0.9096 - 4s/epoch - 26ms/step\n",
      "Epoch 41/50\n",
      "160/160 - 4s - loss: 0.1622 - accuracy: 0.9361 - val_loss: 0.1854 - val_accuracy: 0.9217 - 4s/epoch - 27ms/step\n",
      "Epoch 42/50\n",
      "160/160 - 4s - loss: 0.1567 - accuracy: 0.9416 - val_loss: 0.1725 - val_accuracy: 0.9304 - 4s/epoch - 26ms/step\n",
      "Epoch 43/50\n",
      "160/160 - 4s - loss: 0.1600 - accuracy: 0.9365 - val_loss: 0.2081 - val_accuracy: 0.9252 - 4s/epoch - 27ms/step\n",
      "Epoch 44/50\n",
      "160/160 - 4s - loss: 0.1399 - accuracy: 0.9449 - val_loss: 0.1831 - val_accuracy: 0.9374 - 4s/epoch - 26ms/step\n",
      "Epoch 45/50\n",
      "160/160 - 4s - loss: 0.1397 - accuracy: 0.9504 - val_loss: 0.1645 - val_accuracy: 0.9478 - 4s/epoch - 25ms/step\n",
      "Epoch 46/50\n",
      "160/160 - 4s - loss: 0.1416 - accuracy: 0.9424 - val_loss: 0.1868 - val_accuracy: 0.9443 - 4s/epoch - 26ms/step\n",
      "Epoch 47/50\n",
      "160/160 - 4s - loss: 0.1395 - accuracy: 0.9514 - val_loss: 0.1811 - val_accuracy: 0.9478 - 4s/epoch - 26ms/step\n",
      "Epoch 48/50\n",
      "160/160 - 4s - loss: 0.1429 - accuracy: 0.9467 - val_loss: 0.1544 - val_accuracy: 0.9548 - 4s/epoch - 26ms/step\n",
      "Epoch 49/50\n",
      "160/160 - 4s - loss: 0.1290 - accuracy: 0.9508 - val_loss: 0.1816 - val_accuracy: 0.9374 - 4s/epoch - 25ms/step\n",
      "Epoch 50/50\n",
      "160/160 - 4s - loss: 0.1315 - accuracy: 0.9508 - val_loss: 0.1550 - val_accuracy: 0.9565 - 4s/epoch - 25ms/step\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting CNN\")\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdf75bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
