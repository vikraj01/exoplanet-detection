{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ba60e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import uniform_filter1d, gaussian_filter  # Updated import statements\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    confusion_matrix, fbeta_score, precision_recall_curve,\n",
    "    average_precision_score, auc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "009e59d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv1D, MaxPool1D, Dense, Dropout, Flatten, \\\n",
    "    BatchNormalization, Input, concatenate, Activation\n",
    "from keras.optimizers import Adam\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from inspect import signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7403939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detrender_normalizer(light_flux):\n",
    "  flux1 = light_flux\n",
    "  flux2 = gaussian_filter(flux1, sigma=10)\n",
    "  flux3 = flux1 - flux2\n",
    "  flux3normalized = (flux3-np.mean(flux3)) / (np.max(flux3)-np.min(flux3))\n",
    "  return flux3normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34288e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function for shuffeling in unison\n",
    "def shuffle_in_unison(a, b):\n",
    "  rng_state = np.random.get_state()\n",
    "  np.random.shuffle(a)\n",
    "  np.random.set_state(rng_state)\n",
    "  np.random.shuffle(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fe791ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(x_train, y_train, batch_size=32):\n",
    "    half_batch = batch_size // 2\n",
    "    x_batch = np.empty((batch_size, x_train.shape[1], x_train.shape[2]), dtype='float32') #empty matrix for input\n",
    "    y_batch = np.empty((batch_size, y_train.shape[1]), dtype='float32') #empty matrix for output\n",
    "\n",
    "    # Find indicies for positive and negative labels\n",
    "    while True:\n",
    "        pos_idx = np.where(y_train[:,0] == 1)[0]\n",
    "        neg_idx = np.where(y_train[:,0] == 0)[0]\n",
    "\n",
    "        # Randomize the positive and negative indicies\n",
    "        np.random.shuffle(pos_idx)\n",
    "        np.random.shuffle(neg_idx)\n",
    "\n",
    "        # Let half of the batch have a positive classification and the other\n",
    "        # half have a negative classification\n",
    "        x_batch[:half_batch] = x_train[pos_idx[:half_batch]]\n",
    "        x_batch[half_batch:] = x_train[neg_idx[half_batch:batch_size]] \n",
    "        y_batch[:half_batch] = y_train[pos_idx[:half_batch]]\n",
    "        y_batch[half_batch:] = y_train[neg_idx[half_batch:batch_size]]\n",
    "\n",
    "        # Shuffle batch\n",
    "        shuffle_in_unison(x_batch,y_batch)\n",
    "\n",
    "        # Generating new examples by rotating them in time\n",
    "        for i in range(batch_size):\n",
    "            sz = np.random.randint(x_batch.shape[1])\n",
    "            x_batch[i] = np.roll(x_batch[i], sz, axis = 0)\n",
    "        yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a30b9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Constructing The Neural Networks...\n",
      "Epoch 1/50\n",
      "160/160 - 7s - loss: 0.0440 - accuracy: 0.5000 - val_loss: 4.1416 - val_accuracy: 0.0174 - 7s/epoch - 45ms/step\n",
      "Epoch 2/50\n",
      "160/160 - 5s - loss: 0.0413 - accuracy: 0.5000 - val_loss: 3.9422 - val_accuracy: 0.0174 - 5s/epoch - 29ms/step\n",
      "Epoch 3/50\n",
      "160/160 - 5s - loss: 0.0403 - accuracy: 0.5000 - val_loss: 3.7685 - val_accuracy: 0.0174 - 5s/epoch - 29ms/step\n",
      "Epoch 4/50\n",
      "160/160 - 4s - loss: 0.0394 - accuracy: 0.5000 - val_loss: 3.6706 - val_accuracy: 0.0174 - 4s/epoch - 28ms/step\n",
      "Epoch 5/50\n",
      "160/160 - 5s - loss: 0.0380 - accuracy: 0.5000 - val_loss: 3.4918 - val_accuracy: 0.0174 - 5s/epoch - 29ms/step\n",
      "Epoch 6/50\n",
      "160/160 - 5s - loss: 0.0381 - accuracy: 0.5000 - val_loss: 3.4454 - val_accuracy: 0.0174 - 5s/epoch - 34ms/step\n",
      "Epoch 7/50\n",
      "160/160 - 5s - loss: 0.0368 - accuracy: 0.5000 - val_loss: 3.3183 - val_accuracy: 0.0174 - 5s/epoch - 33ms/step\n",
      "Epoch 8/50\n",
      "160/160 - 5s - loss: 0.0367 - accuracy: 0.5000 - val_loss: 3.2468 - val_accuracy: 0.0174 - 5s/epoch - 31ms/step\n",
      "Epoch 9/50\n",
      "160/160 - 5s - loss: 0.0360 - accuracy: 0.5000 - val_loss: 3.1703 - val_accuracy: 0.0174 - 5s/epoch - 28ms/step\n",
      "Epoch 10/50\n",
      "160/160 - 5s - loss: 0.0352 - accuracy: 0.5000 - val_loss: 3.0813 - val_accuracy: 0.0174 - 5s/epoch - 29ms/step\n",
      "Epoch 11/50\n",
      "160/160 - 5s - loss: 0.0343 - accuracy: 0.5000 - val_loss: 3.0541 - val_accuracy: 0.0174 - 5s/epoch - 29ms/step\n",
      "Epoch 12/50\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Converting the format from a dataframe to numpy arrays (matrices)\n",
    "    # and defining x-values and y-values for both the test and training set\n",
    "    print(\"Loading datasets...\")\n",
    "    train = pd.read_csv(\"Data/exoTrain.csv\", encoding=\"ISO-8859-1\")  # in dataframe format\n",
    "    test = pd.read_csv(\"Data/exoTest.csv\", encoding=\"ISO-8859-1\")  # in dataframe format\n",
    "    x_train = train.drop('LABEL', axis=1)\n",
    "    x_test = test.drop('LABEL', axis=1)\n",
    "    y_train = train.LABEL\n",
    "    y_test = test.LABEL\n",
    "    x_train = np.array(x_train)\n",
    "    y_train = np.array(y_train).reshape((-1, 1)) - 1\n",
    "    x_test = np.array(x_test)\n",
    "    y_test = np.array(y_test).reshape((-1, 1)) - 1\n",
    "\n",
    "    # Add extra positive examples of light curves by flipping them ( Data Augmentation )\n",
    "    x_train = np.append(x_train, np.flip(x_train[0:37, :], axis=-1), axis=0)\n",
    "    y_train = np.append(y_train, y_train[0:37]).reshape((-1, 1))\n",
    "    x_test = np.append(x_test, np.flip(x_test[0:5, :], axis=-1), axis=0)\n",
    "    y_test = np.append(y_test, y_test[0:5]).reshape((-1, 1))\n",
    "\n",
    "    # Detrend the data sets\n",
    "    x_train_p = detrender_normalizer(x_train)\n",
    "    x_test_p = detrender_normalizer(x_test)\n",
    "\n",
    "    # Scale each observation to zero mean and unit variance\n",
    "    x_train = ((x_train - np.mean(x_train, axis=1).reshape(-1, 1)) / np.std(x_train, axis=1).reshape(-1, 1))\n",
    "    x_test = ((x_test - np.mean(x_test, axis=1).reshape(-1, 1)) / np.std(x_test, axis=1).reshape(-1, 1))\n",
    "\n",
    "    # Stack the zero mean unit variance normalized data and the detrended data\n",
    "    x_train = np.stack([x_train, x_train_p], axis=2)\n",
    "    x_test = np.stack([x_test, x_test_p], axis=2)\n",
    "\n",
    "    print(\"Constructing The Neural Networks...\")\n",
    "    # Construct the neural network\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=8, kernel_size=11, activation='linear', input_shape=x_train.shape[1:]))\n",
    "    model.add(MaxPool1D(strides=4))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=16, kernel_size=11, activation='relu'))\n",
    "    model.add(MaxPool1D(strides=4))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=32, kernel_size=11, activation='relu'))\n",
    "    model.add(MaxPool1D(strides=4))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=64, kernel_size=11, activation='relu'))\n",
    "    model.add(MaxPool1D(strides=4))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))  # prevents overfitting\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.25))  # prevents overfitting\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    class_weights = {0: 0.01465347, 1: 1.0}\n",
    "    \n",
    "    # Compile model and train the model, make sure it converges\n",
    "    model.compile(optimizer=Adam(1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    hist = model.fit_generator(batch_generator(x_train, y_train, 32),\n",
    "                               validation_data=(x_test, y_test),\n",
    "                               verbose=0, epochs=5,\n",
    "                               steps_per_epoch=x_train.shape[0] // 32, class_weight = class_weights)\n",
    "\n",
    "    # Proceeding the training with a faster learning rate\n",
    "    model.compile(optimizer=Adam(4e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    hist = model.fit_generator(batch_generator(x_train, y_train, 32),\n",
    "                              validation_data=(x_test, y_test),\n",
    "                              verbose=2, epochs=50,\n",
    "                              steps_per_epoch=x_train.shape[0] // 32, class_weight = class_weights)\n",
    "\n",
    "    # Saving model to JSON and weights to HDF5\n",
    "    model_json = model.to_json()\n",
    "    with open(\"model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(\"model.h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "    # Plot the loss and accuracy for the training\n",
    "    plt.plot(hist.history['loss'], color='b', label='loss')\n",
    "    plt.plot(hist.history['val_loss'], color='r', label='validation loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    plt.plot(hist.history['accuracy'], color='b', label='accuracy')\n",
    "    plt.plot(hist.history['val_accuracy'], color='r', label='validation accuracy')\n",
    "\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "    # Make predictions for training data\n",
    "    shuffle_in_unison(x_train, y_train)\n",
    "    y_pred = model.predict(x_train)[:, 0]\n",
    "    pred = np.empty((1, len(y_pred)), dtype=object)\n",
    "    pred = np.where(y_pred >= 0.5, 1, 0)\n",
    "    y_train = np.reshape(y_train, len(y_train))\n",
    "    pred = np.reshape(pred, len(pred))\n",
    "\n",
    "    # Create confusion matrix for training data\n",
    "    print('Validation for training data:')\n",
    "    conf_matrix = pd.crosstab(y_train, pred)\n",
    "    print(conf_matrix)\n",
    "\n",
    "    # Calculate precision and recall\n",
    "    accuracy = accuracy_score(y_train, pred)\n",
    "    precision = precision_score(y_train, pred)\n",
    "    recall = recall_score(y_train, pred)\n",
    "    fbeta = fbeta_score(y_train, pred, beta=1)\n",
    "    print('Accuracy: %.3f Precision: %.3f Recall: %.3f F_beta: %.3f' % (accuracy, precision, recall, fbeta))\n",
    "\n",
    "    # Create a precision recall curve\n",
    "    precision, recall, thresholds = precision_recall_curve(y_train, y_pred, pos_label=1)\n",
    "    auc_pr = auc(recall, precision)\n",
    "    print('Area under precision-recall-curve: %.3f' % (auc_pr))\n",
    "    step_kwargs = ({'step': 'post'}\n",
    "                   if 'step' in signature(plt.fill_between).parameters\n",
    "                   else {})\n",
    "    plt.step(recall, precision, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.show()\n",
    "\n",
    "    # Make predictions for test data\n",
    "    shuffle_in_unison(x_test, y_test)\n",
    "    y_pred = model.predict(x_test)[:, 0]\n",
    "    pred = np.empty((1, len(y_pred)), dtype=object)\n",
    "    pred = np.where(y_pred >= 0.5, 1, 0)\n",
    "    y_test = np.reshape(y_test, len(y_test))\n",
    "    pred = np.reshape(pred, len(pred))\n",
    "\n",
    "    # Create confusion matrix for test data\n",
    "    print('Validation for test data:')\n",
    "    conf_matrix = pd.crosstab(y_test, pred)\n",
    "    print(conf_matrix)\n",
    "\n",
    "    # Calculate precision and recall\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    precision = precision_score(y_test, pred)\n",
    "    recall = recall_score(y_test, pred)\n",
    "    fbeta = fbeta_score(y_test, pred, beta=1)\n",
    "    print('Accuracy: %.3f Precision: %.3f Recall: %.3f F_beta: %.3f' % (accuracy, precision, recall, fbeta))\n",
    "\n",
    "    # Create a precision recall curve\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred, pos_label=1)\n",
    "    auc_pr = auc(recall, precision)\n",
    "    print('Area under precision-recall-curve: %.3f' % (auc_pr))\n",
    "    step_kwargs = ({'step': 'post'}\n",
    "                   if 'step' in signature(plt.fill_between).parameters\n",
    "                   else {})\n",
    "    plt.step(recall, precision, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.show()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf9c4e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28279eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46567b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
