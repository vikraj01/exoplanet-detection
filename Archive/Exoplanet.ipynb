{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b6b2254965f03e535cb73f1abf3bba4ec5c3cfe4"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "from pathlib import Path\n",
    "from scipy import ndimage, fft\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, precision_recall_curve, average_precision_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.utils import shuffle\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d28cb86655c3af600b8377e7de783c488c2e3676"
   },
   "source": [
    "## Data Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "87e08df975d26a6d05feb1005931fa20ad450fde"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LightFluxProcessor:\n",
    "\n",
    "    def __init__(self, fourier=True, normalize=True, gaussian=True, standardize=True):\n",
    "        self.fourier = fourier\n",
    "        self.normalize = normalize\n",
    "        self.gaussian = gaussian\n",
    "        self.standardize = standardize\n",
    "\n",
    "    def fourier_transform(self, X):\n",
    "        return np.abs(np.fft.fft(X, n=X.size))\n",
    "\n",
    "    def process(self, df_train_x, df_dev_x):\n",
    "        # Apply fourier transform\n",
    "        if self.fourier:\n",
    "            print(\"Applying Fourier...\")\n",
    "            df_train_x = np.apply_along_axis(self.fourier_transform, axis=1, arr=df_train_x)\n",
    "            df_dev_x = np.apply_along_axis(self.fourier_transform, axis=1, arr=df_dev_x)\n",
    "\n",
    "            # Keep the first half of data as it is symmetrical after the previous steps\n",
    "            df_train_x = df_train_x[:, :(df_train_x.shape[1] // 2)]\n",
    "            df_dev_x = df_dev_x[:, :(df_dev_x.shape[1] // 2)]\n",
    "\n",
    "        # Normalize\n",
    "        if self.normalize:\n",
    "            print(\"Normalizing...\")\n",
    "            df_train_x = normalize(df_train_x)\n",
    "            df_dev_x = normalize(df_dev_x)\n",
    "\n",
    "        # Gaussian filter to smooth out data\n",
    "        if self.gaussian:\n",
    "            print(\"Applying Gaussian Filter...\")\n",
    "            df_train_x = ndimage.filters.gaussian_filter(df_train_x, sigma=10)\n",
    "            df_dev_x = ndimage.filters.gaussian_filter(df_dev_x, sigma=10)\n",
    "\n",
    "        if self.standardize:\n",
    "            # Standardize X data\n",
    "            print(\"Standardizing...\")\n",
    "            std_scaler = StandardScaler()\n",
    "            df_train_x = std_scaler.fit_transform(df_train_x)\n",
    "            df_dev_x = std_scaler.transform(df_dev_x)\n",
    "\n",
    "        print(\"Finished Processing!\")\n",
    "        return df_train_x, df_dev_x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c17634d1b3ecb3fd7b361d19c8774df26dac4ea2"
   },
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "8aef4d6898bf06a2414a3a675a1fa9400f97cbd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Loaded datasets!\n"
     ]
    }
   ],
   "source": [
    "train_dataset_path = \"Data/exoTrain.csv\"\n",
    "dev_dataset_path = \"Data/exoTest.csv\"\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "df_train = pd.read_csv(train_dataset_path, encoding = \"ISO-8859-1\")\n",
    "df_dev = pd.read_csv(dev_dataset_path, encoding = \"ISO-8859-1\")\n",
    "print(\"Loaded datasets!\")\n",
    "\n",
    "# Generate X and Y dataframe sets\n",
    "df_train_x = df_train.drop('LABEL', axis=1)\n",
    "df_dev_x = df_dev.drop('LABEL', axis=1)\n",
    "df_train_y = df_train.LABEL\n",
    "df_dev_y = df_dev.LABEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3e03e00f593729a6fb36841cde640a0cb366bf4f"
   },
   "source": [
    "### Process data and create numpy matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "504e0ecddfafc27cdf49022f2950566373c0d6fe"
   },
   "outputs": [],
   "source": [
    "def np_X_Y_from_df(df):\n",
    "    df = shuffle(df)\n",
    "    df_X = df.drop(['LABEL'], axis=1)\n",
    "    X = np.array(df_X)\n",
    "    Y_raw = np.array(df['LABEL']).reshape((len(df['LABEL']),1))\n",
    "    Y = Y_raw == 2\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "90a0ea43b0ef441d5ff094adec4d6a1e2030ab58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Fourier...\n",
      "Finished Processing!\n"
     ]
    }
   ],
   "source": [
    "# Process dataset\n",
    "LFP = LightFluxProcessor(\n",
    "    fourier=True,\n",
    "    normalize=False,\n",
    "    gaussian=False,\n",
    "    standardize=False)\n",
    "df_train_x, df_dev_x = LFP.process(df_train_x, df_dev_x)\n",
    "\n",
    "# Rejoin X and Y\n",
    "df_train_processed = pd.DataFrame(df_train_x).join(pd.DataFrame(df_train_y))\n",
    "df_dev_processed = pd.DataFrame(df_dev_x).join(pd.DataFrame(df_dev_y))\n",
    "\n",
    "# Load X and Y numpy arrays\n",
    "X_train, Y_train = np_X_Y_from_df(df_train_processed)\n",
    "X_dev, Y_dev = np_X_Y_from_df(df_dev_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0eb999422374f71b0f44292382dee19239332f37"
   },
   "source": [
    "### Describe datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "744f7863433eb3b36ca7e93abd2311dd658c790b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (5087, 1598)\n",
      "Y_train.shape:  (5087, 1)\n",
      "X_dev.shape:  (570, 1598)\n",
      "Y_dev.shape:  (570, 1)\n",
      "n_x:  1598\n",
      "num_examples:  5087\n",
      "n_y:  1\n"
     ]
    }
   ],
   "source": [
    "(num_examples, n_x) = X_train.shape # (n_x: input size, m : number of examples in the train set)\n",
    "n_y = Y_train.shape[1] # n_y : output size\n",
    "print(\"X_train.shape: \", X_train.shape)\n",
    "print(\"Y_train.shape: \", Y_train.shape)\n",
    "print(\"X_dev.shape: \", X_dev.shape)\n",
    "print(\"Y_dev.shape: \", Y_dev.shape)\n",
    "print(\"n_x: \", n_x)\n",
    "print(\"num_examples: \", num_examples)\n",
    "print(\"n_y: \", n_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b4783cdb1902414c346e39c709b5619788e4b748"
   },
   "source": [
    "## Build Model, Train, and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "cea07ef250b87a2d326410dd5985e9cec1517c46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikas\\Desktop\\exoplanet\\env\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikas\\Desktop\\exoplanet\\env\\lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = LinearSVC()\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_sm, Y_train_sm = sm.fit_resample(X_train, Y_train)\n",
    "# X_train_sm, Y_train_sm = X_train, Y_train\n",
    "\n",
    "# Train\n",
    "print(\"Training...\")\n",
    "model.fit(X_train_sm, Y_train_sm)\n",
    "\n",
    "train_outputs = model.predict(X_train_sm)\n",
    "dev_outputs = model.predict(X_dev)\n",
    "print(\"Finished Training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1d89125e42171a7bfda48da4313b92ca0b04d866"
   },
   "source": [
    "## Calculate and Display Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "9e3b8584de8273df5d255d6cd0eebbb0590c458c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      "Train Set Error 0.0013861386138613874\n",
      "Dev Set Error 0.010526315789473717\n",
      "------------\n",
      "Precision - Train Set 0.9972353870458136\n",
      "Precision - Dev Set 0.42857142857142855\n",
      "------------\n",
      "Recall - Train Set 1.0\n",
      "Recall - Dev Set 0.6\n",
      "------------\n",
      "Confusion Matrix - Train Set\n",
      "[[5036   14]\n",
      " [   0 5050]]\n",
      "Confusion Matrix - Dev Set\n",
      "[[561   4]\n",
      " [  2   3]]\n",
      "------------\n",
      " \n",
      " \n",
      "------------\n",
      "classification_report_train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      1.00      1.00      5050\n",
      "        True       1.00      1.00      1.00      5050\n",
      "\n",
      "    accuracy                           1.00     10100\n",
      "   macro avg       1.00      1.00      1.00     10100\n",
      "weighted avg       1.00      1.00      1.00     10100\n",
      "\n",
      "classification_report_dev\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.99      0.99       565\n",
      "        True       0.43      0.60      0.50         5\n",
      "\n",
      "    accuracy                           0.99       570\n",
      "   macro avg       0.71      0.80      0.75       570\n",
      "weighted avg       0.99      0.99      0.99       570\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Metrics\n",
    "train_outputs = model.predict(X_train_sm)\n",
    "dev_outputs = model.predict(X_dev)\n",
    "train_outputs = np.rint(train_outputs)\n",
    "dev_outputs = np.rint(dev_outputs)\n",
    "accuracy_train = accuracy_score(Y_train_sm, train_outputs)\n",
    "accuracy_dev = accuracy_score(Y_dev, dev_outputs)\n",
    "precision_train = precision_score(Y_train_sm, train_outputs)\n",
    "precision_dev = precision_score(Y_dev, dev_outputs)\n",
    "recall_train = recall_score(Y_train_sm, train_outputs)\n",
    "recall_dev = recall_score(Y_dev, dev_outputs)\n",
    "confusion_matrix_train = confusion_matrix(Y_train_sm, train_outputs)\n",
    "confusion_matrix_dev = confusion_matrix(Y_dev, dev_outputs)\n",
    "classification_report_train = classification_report(Y_train_sm, train_outputs)\n",
    "classification_report_dev = classification_report(Y_dev, dev_outputs)\n",
    "\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\"Train Set Error\", 1.0 - accuracy_train)\n",
    "print(\"Dev Set Error\", 1.0 - accuracy_dev)\n",
    "print(\"------------\")\n",
    "print(\"Precision - Train Set\", precision_train)\n",
    "print(\"Precision - Dev Set\", precision_dev)\n",
    "print(\"------------\")\n",
    "print(\"Recall - Train Set\", recall_train)\n",
    "print(\"Recall - Dev Set\", recall_dev)\n",
    "print(\"------------\")\n",
    "print(\"Confusion Matrix - Train Set\")\n",
    "print(confusion_matrix_train)\n",
    "print(\"Confusion Matrix - Dev Set\")\n",
    "print(confusion_matrix_dev)\n",
    "print(\"------------\")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\"------------\")\n",
    "print(\"classification_report_train\")\n",
    "print(classification_report_train)\n",
    "print(\"classification_report_dev\")\n",
    "print(classification_report_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [10100, 5087]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m train_outputs \u001b[38;5;241m=\u001b[39m (train_prob \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m      7\u001b[0m dev_outputs \u001b[38;5;241m=\u001b[39m (dev_prob \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m accuracy_train \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_train_sm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m accuracy_dev \u001b[38;5;241m=\u001b[39m accuracy_score(Y_dev, dev_outputs)\n\u001b[0;32m     11\u001b[0m precision_train \u001b[38;5;241m=\u001b[39m precision_score(Y_train_sm, train_outputs)\n",
      "File \u001b[1;32m~\\Desktop\\exoplanet\\env\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    212\u001b[0m         )\n\u001b[0;32m    213\u001b[0m     ):\n\u001b[1;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    224\u001b[0m     )\n",
      "File \u001b[1;32m~\\Desktop\\exoplanet\\env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:220\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 220\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\Desktop\\exoplanet\\env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:84\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     58\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     86\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Desktop\\exoplanet\\env\\lib\\site-packages\\sklearn\\utils\\validation.py:407\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    405\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    408\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    409\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    410\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [10100, 5087]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Metrics\n",
    "train_prob = model.decision_function(X_train)  # Use decision_function for linear SVM\n",
    "dev_prob = model.decision_function(X_dev)\n",
    "\n",
    "# Calculate binary predictions (0 or 1) based on the decision function\n",
    "train_outputs = (train_prob > 0).astype(int)\n",
    "dev_outputs = (dev_prob > 0).astype(int)\n",
    "accuracy_train = accuracy_score(Y_train_sm, train_outputs)\n",
    "accuracy_dev = accuracy_score(Y_dev, dev_outputs)\n",
    "precision_train = precision_score(Y_train_sm, train_outputs)\n",
    "precision_dev = precision_score(Y_dev, dev_outputs)\n",
    "recall_train = recall_score(Y_train_sm, train_outputs)\n",
    "recall_dev = recall_score(Y_dev, dev_outputs)\n",
    "confusion_matrix_train = confusion_matrix(Y_train_sm, train_outputs)\n",
    "confusion_matrix_dev = confusion_matrix(Y_dev, dev_outputs)\n",
    "classification_report_train = classification_report(Y_train_sm, train_outputs)\n",
    "classification_report_dev = classification_report(Y_dev, dev_outputs)\n",
    "\n",
    "# Calculate AUC scores\n",
    "ap_train = average_precision_score(Y_train_sm, train_prob)\n",
    "ap_dev = average_precision_score(Y_dev, dev_prob)\n",
    "\n",
    "# Display metrics\n",
    "print(\"AUC training set: %.3f\" % ap_train)\n",
    "print(\"AUC dev set: %.3f\" % ap_dev)\n",
    "print(\"Accuracy training set: %.3f\" % accuracy_train)\n",
    "print(\"Accuracy dev set: %.3f\" % accuracy_dev)\n",
    "print(\"Precision training set: %.3f\" % precision_train)\n",
    "print(\"Precision dev set: %.3f\" % precision_dev)\n",
    "print(\"Recall training set: %.3f\" % recall_train)\n",
    "print(\"Recall dev set: %.3f\" % recall_dev)\n",
    "print(\" \")\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision, recall, thresholds = precision_recall_curve(Y_train_sm, train_prob)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, color='b', alpha=0.8)\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='b', step='post')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrices\n",
    "print(\"Confusion Matrix - Train Set\")\n",
    "print(confusion_matrix_train)\n",
    "print(\"Confusion Matrix - Dev Set\")\n",
    "print(confusion_matrix_dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
